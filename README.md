<br>
<p align="center">
<h1 align="center"><strong>SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model
</strong></h1>
  <p align="center">
      <strong><span style="color: red;">CVPR 2025</span></strong>
    <br>
   Chunlin Yu*</a>&emsp;
    <a href='https://hq-King.github.io' target='_blank'>Hanqing Wang*</a>&emsp;
   Ye Shi</a>&emsp;
   Haoyang Luo</a>&emsp;
    Sibei Yang</a>&emsp;
   Jingyi Yu</a>&emsp;
   Jingya Wang</a>&emsp;
    <br>
    ShanghaiTech University    
    <br>
    *Indicates Equal Contribution
    <br>
  </p>
</p>

  

<p align="center">
  <a href="https://seq-afford.github.io"><b>ğŸ“– Project Page</b></a> |
  <a href="https://arxiv.org/pdf/2412.01550"><b>ğŸ“„ Paper Link</b></a> |
</p>

</div>



> We introduce SeqAfford, a Multi-Modal Language Model (MLLM) capable of serialized affordance inference implied in human instructions: 1) Single Affordance Reasoning; 2) Sequential Affordance Reasoning; 3) Sequential Affordance Reasoning with Multiple Objects

<div align="center">
    <img src="fig1.png" height=500>
</div>

## I have quit the graduate program @ShanghaiTech Unversity in March 2024 while I am the co-first author, and therefore do not have the final version of the code. The code in this repository is my preliminary code from around September 2024, with some details potentially missing. However, to help the community follow up on our research, I have uploaded it here. But I noticed that the VDI group member@ShanghaiTech [Zhenhao Zhang](https://zhangzhh.cn/) have utilized this model to help with HOI generation, so you can check the details in his repo [OpenHOI](https://github.com/Zhenhao-Zhang/OpenHOI), which was accepted by NeurIPS 2025 as an oral paper.

## ğŸ“£ News

- [2/27/2025] ğŸ‰ğŸ‰ğŸ‰SeqAfford has been accepted by CVPR 2025!!!ğŸ‰ğŸ‰ğŸ‰
- [12/2/2024] SeqAfford has been released on Arxiv now!!!

## ğŸ˜² Results
Please refer to our [homepage](https://seq-afford.github.io) for more thrilling results!


## ğŸ› ï¸ Setup
- 1. Create a new `conda` environment and activate it by following command
  ```bash
  conda env create -f environment.yaml
  ```
- 2. Down [ShapeLLM](https://github.com/qizekun/ShapeLLM/blob/main/docs/MODEL_ZOO.md) model weight into your directory, and Modify the model path in the `scripts/finetune_lora.sh`ï¼Œ including both `--vision_tower_path` and `--pretrain_mm_mlp_adapter`
 
 - 3. Down [Uni3D](https://github.com/baaivision/Uni3D) model weight into your directory, and Modify the model path in the `./llava/model/language_model/affordancellm.py`
 
- 4. you can train your own model by running the following code
 
```bash
  sh ./scripts/finetune_lora.sh
```
# ğŸ“š Data
visit the link to download the [Dataset](https://pan.baidu.com/s/1_koVmNMdv5BByli97eDHGA?pwd=2025)

## ğŸš© Plan
- [x] Paper Released.
- [âˆš ] Source Code and Pretrained Weights.
- [âˆš ] Dataset.
<!-- --- -->

## Acknowledgement
Thanks for the wonderful works: [ShapeLLM](https://github.com/qizekun/ShapeLLM/blob/main/docs/MODEL_ZOO.md), [LISA](https://github.com/dvlab-research/LISA), This work is built upon them.

## ğŸ« License

For academic use, this project is licensed under [the 2-clause BSD License](https://opensource.org/license/bsd-2-clause). 

## ğŸ–Šï¸ Citation
```
@article{yu2024seqafford,
        title={SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model},
        author={Yu, Chunlin and Wang, Hanqing and Shi, Ye and Luo, Haoyang and Yang, Sibei and Yu, Jingyi and Wang, Jingya},
        journal={arXiv preprint arXiv:2412.01550},
        year={2024}
      }

```
